# Gesture Recognition (MediaPipe + PyTorch)

This project is a **standalone, desktop gesture recognition system** built in Python.

It uses:

- **MediaPipe** â†’ fast, realâ€‘time hand landmark detection
- **PyTorch** â†’ machineâ€‘learning classifier for static hand gestures (frameâ€‘byâ€‘frame)

The output is a **predicted gesture label with confidence**, displayed on screen and printed to the terminal.

This project is intentionally designed as a **foundation** for later extensions (e.g. drone control), while remaining safe, debuggable, and modular.

---

## Project Goals

- Realâ€‘time hand gesture recognition using a webcam
- Clean separation between:
  - perception (hand tracking)
  - learning (gesture classification)
  - decision logic (what to do with the gesture)
- Robust to hand size, position, and scale
- Easy to extend with new gestures

---

## Gesture Set

Current gestures (8 classes):

1. `POINT_FORWARD`
2. `PEACE_BACK` (peace sign with **back of hand** facing camera)
3. `THUMBS_LEFT`
4. `PINKY_RIGHT`
5. `OPEN_UP`
6. `FIST_DOWN`
7. `OKAY_LAND`
8. `LOVE_ROTATE`

> âš ï¸ Gestures are **static** (single frame), not dynamic sequences.

---

## Highâ€‘Level Architecture

```
Webcam
  â†“
MediaPipe Hands
  â†“
21 hand landmarks (x, y, z)
  â†“
Feature normalization (translation + scale)
  â†“
PyTorch MLP classifier
  â†“
Gesture label + confidence
```

Key idea:

> **MediaPipe finds the hand. PyTorch decides what it means.**

---

## Folder Structure

```
gesture-recognizer/
â”œâ”€ .venv/                # Virtual environment (recommended)
â”œâ”€ collect_data.py       # Webcam data collection tool
â”œâ”€ train.py              # PyTorch training script
â”œâ”€ infer.py              # Live inference (webcam)
â”œâ”€ utils.py              # Shared utilities (features, labels)
â”œâ”€ data/
â”‚   â””â”€ raw/              # Collected training samples (.npz)
â”œâ”€ models/
â”‚   â””â”€ model.pth         # Trained PyTorch model
â””â”€ README.md
```

---

## Environment Setup

### Create virtual environment (recommended)

From inside the project folder:

```bash
py -3.10 -m venv .venv
.venv\Scripts\activate
```

### Install dependencies

```bash
python -m pip install --upgrade pip
pip install mediapipe==0.10.8 opencv-python numpy torch torchvision torchaudio
```

Verify:

```bash
python -c "import mediapipe as mp, torch; print(hasattr(mp,'solutions'), torch.__version__)"
```

---

## How It Works (Key Design Choices)

### 1. Landmark Features

- 21 hand landmarks Ã— (x, y, z) â†’ **63 values**
- Wrist landmark is used as origin (translation invariance)
- Scaled by max distance to wrist (scale invariance)
- +1 handedness bit (Left / Right)

**Final feature vector: 64 dimensions**

This allows the model to learn:

- hand orientation
- finger configuration
- left/right differences

---

### 2. Classifier

- Feedâ€‘forward **MLP (Multiâ€‘Layer Perceptron)**
- Trained with crossâ€‘entropy loss
- Dropout used during training for robustness

This is intentionally simple:

- fast inference
- easy debugging
- good performance for static gestures

---

## Usage

### 1ï¸âƒ£ Collect training data

```bash
python collect_data.py
```

Controls:

- `1..8` â†’ select gesture class
- `S` â†’ save a sample
- `Q` â†’ quit

Recommendations:

- Collect **300â€“800 samples per gesture**
- Vary:
  - distance from camera
  - slight wrist rotation
  - lighting conditions
- For `PEACE_BACK`, ensure the **back of the hand** faces the camera

---

### 2ï¸âƒ£ Train the model

```bash
python train.py
```

Output:

- Training + validation accuracy per epoch
- Best model saved to:

```
models/model.pth
```

> âš ï¸ Training will warn you if a gesture has **0 samples** â€” fix this before inference.

---

### 3ï¸âƒ£ Run live inference

```bash
python infer.py
```

Features:

- Realâ€‘time webcam prediction
- Confidence thresholding
- Terminal output **only when prediction changes**
- Onâ€‘screen gesture + confidence display

Press `Q` to quit.

---

## Safety & Robustness Improvements (Important)

Even though this is a desktop project, the following best practices are already applied:

### âœ” Confidence threshold

Lowâ€‘confidence predictions are ignored (`UNSURE`).

### âœ” Cooldown

Prevents rapid flickering / spam output.

### âœ” Architecture consistency

Training and inference models **must match exactly**.

### âœ” Explicit gesture set

No hidden magic labels.

---

## Known Limitations

- Static gestures only (no motionâ€‘based gestures yet)
- Requires retraining if gesture set changes
- Accuracy depends heavily on quality & balance of data

---

## Why This Design Scales Well

This project cleanly separates:

- **Perception** â†’ MediaPipe
- **Learning** â†’ PyTorch
- **Decision logic** â†’ your code

This makes it easy to:

- add gesture smoothing
- add temporal models (LSTM / Transformer)
- replace terminal output with:
  - keyboard events
  - robot commands
  - drone SDK commands (later)

---

## Next Possible Extensions

- Temporal smoothing (majority vote over N frames)
- Dynamic gesture recognition (sequences)
- Gestureâ€‘toâ€‘command mapping with safety states
- Export model for deployment

---

## Summary

This miniâ€‘project is a **complete, realâ€‘world gesture recognition pipeline**:

- fast
- modular
- debuggable
- extensible

It is suitable as:

- a learning project
- a research prototype
- the perception layer of a robotics or drone system

---

Happy hacking âœ‹ğŸš€
